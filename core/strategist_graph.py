"""
Patent-DeepScientist ç³»ç»Ÿ - Idea æå‡ºæ¨¡å— V4.0
ä½¿ç”¨ LangGraph æ¡†æ¶å®ç°çŸ¥è¯†æ£€ç´¢å’Œç ”ç©¶æ–¹æ¡ˆç”Ÿæˆ

âœ¨ V4.0 æ ¸å¿ƒä¼˜åŒ– (2025-12-05):
1. æ„å›¾è½¬è¯‘ (Intent Translation): LLM è‡ªåŠ¨æå–æ£€ç´¢å…³é”®è¯ï¼Œæé«˜å‘½ä¸­ç‡
2. è¯¦ç»†é…ç½®æå–: Cypher æŸ¥è¯¢è¿”å› config å’Œ metricsï¼Œæ–¹æ¡ˆæ›´å…·å¯æ‰§è¡Œæ€§
3. è·¨åŸŸè¿ç§» Prompt: å¼ºåŒ–ç±»æ¯”æ¨ç†ï¼Œå®ç°çœŸæ­£çš„"ä¸¾ä¸€åä¸‰"
4. è´¨é‡æ£€æŸ¥èŠ‚ç‚¹: è‡ªåŠ¨è¯„ä»·æ–¹æ¡ˆè´¨é‡ï¼Œä¸åˆæ ¼åˆ™é‡æ–°ç”Ÿæˆ

ä¼˜åŒ–å»ºè®®æ¥æº: geimin
"""

import os
import json
from typing import TypedDict, List, Dict, Any
from dotenv import load_dotenv

from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from neo4j import GraphDatabase

from neo4j_config import NEO4J_CONFIG

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ============================================================================
# 1. å®šä¹‰çŠ¶æ€ (State)
# ============================================================================

class AgentState(TypedDict):
    """Agent å·¥ä½œæµçŠ¶æ€"""
    user_goal: str              # ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ç›®æ ‡
    graph_context: str          # ä»çŸ¥è¯†å›¾è°±æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    generated_idea: dict        # ç”Ÿæˆçš„ç ”ç©¶æ–¹æ¡ˆ (JSON)
    critique: str               # è‡ªæˆ‘åæ€/è¯„ä»·
    quality_passed: bool        # âœ¨ è´¨é‡æ£€æŸ¥æ˜¯å¦é€šè¿‡
    iteration_count: int        # âœ¨ è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰


# ============================================================================
# 2. Neo4j çŸ¥è¯†å›¾è°±å·¥å…·
# ============================================================================

class GraphTool:
    """Neo4j çŸ¥è¯†å›¾è°±æŸ¥è¯¢å·¥å…·"""
    
    def __init__(self):
        """åˆå§‹åŒ– Neo4j é©±åŠ¨"""
        self.driver = GraphDatabase.driver(
            NEO4J_CONFIG["uri"],
            auth=(NEO4J_CONFIG["user"], NEO4J_CONFIG["password"])
        )
        print("âœ“ Neo4j è¿æ¥å·²å»ºç«‹")
    
    def close(self):
        """å…³é—­è¿æ¥"""
        self.driver.close()
    
    def run_cypher(self, query: str, parameters: dict = None) -> List[Dict]:
        """
        æ‰§è¡Œ Cypher æŸ¥è¯¢
        
        Args:
            query: Cypher æŸ¥è¯¢è¯­å¥
            parameters: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        with self.driver.session() as session:
            result = session.run(query, parameters or {})
            return [dict(record) for record in result]
    
    def retrieve_best_practices(self, keyword: str, limit: int = 3) -> List[Dict]:
        """
        æ£€ç´¢æœ€ä½³å®è·µæ¡ˆä¾‹
        
        âœ¨ V4.1 ä¼˜åŒ–ï¼šå…¨é“¾æ£€ç´¢ (Full Logic Chain Retrieval)
        ä¸€æ—¦æŸç¯‡è®ºæ–‡çš„æŸä¸ªæ­¥éª¤å‘½ä¸­äº†å…³é”®è¯ï¼Œå°±è¿”å›è¯¥è®ºæ–‡çš„ã€å®Œæ•´åˆ†æé€»è¾‘é“¾ã€‘ã€‚
        è¿™æ ·æ™ºèƒ½ä½“æ‰èƒ½å­¦åˆ° Step 1 -> Step 2 -> Step 3 çš„å®Œæ•´æµç¨‹ã€‚
        
        ä¼˜åŒ–å»ºè®®æ¥æº: geimin
        """
        query = """
        // 1. é”šå®šï¼šå…ˆæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„é‚£ä¸ªå…·ä½“æ­¥éª¤ï¼Œé”å®šå¯¹åº”çš„è®ºæ–‡
        MATCH (p:Paper)-[:CONDUCTS]->(target_ae:AnalysisEvent)
        WHERE target_ae.objective CONTAINS $keyword 
           OR p.title CONTAINS $keyword
           OR target_ae.method_name CONTAINS $keyword
        
        // 2. æ‰©å±•ï¼šåŸºäºæ‰¾åˆ°çš„è®ºæ–‡ï¼ŒæŠŠå®ƒæ‰€æœ‰çš„æ­¥éª¤éƒ½æ‰¾å‡ºæ¥
        WITH DISTINCT p
        MATCH (p)-[:CONDUCTS]->(all_ae:AnalysisEvent)
        
        // 3. å…³è”ï¼šè·å–æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ï¼ˆæ–¹æ³•ã€æ•°æ®ã€ç»“è®ºï¼‰
        OPTIONAL MATCH (all_ae)-[:EXECUTES]->(m:Method)
        OPTIONAL MATCH (all_ae)-[:YIELDS]->(c:Conclusion)
        OPTIONAL MATCH (d:Data)-[:FEEDS_INTO]->(all_ae)
        
        // 4. èšåˆï¼šæŒ‰ step_id æ’åºï¼Œé‡ç»„ä¸ºå®Œæ•´çš„ Story
        WITH p, all_ae, m, c, collect(DISTINCT d.name) AS data_fields
        ORDER BY all_ae.step_id ASC
        
        // 5. è¿”å›ç»“æ„åŒ–æ•°æ®ï¼šä¸€ç¯‡è®ºæ–‡ä¸€è¡Œï¼ŒåŒ…å«ä¸€ä¸ª steps æ•°ç»„
        RETURN 
            p.title AS paper_title,
            p.year AS paper_year,
            collect({
                step_id: all_ae.step_id,
                objective: all_ae.objective,
                method_name: all_ae.method_name,
                method: m.name,
                config: all_ae.config,
                metrics: all_ae.metrics,
                inputs: data_fields,
                conclusion_type: c.type,
                conclusion: c.content
            }) AS full_logic_chain
        ORDER BY p.year DESC
        LIMIT $limit
        """
        
        return self.run_cypher(query, {"keyword": keyword, "limit": limit})
    
    def retrieve_research_gaps(self, limit: int = 3) -> List[Dict]:
        """
        æ£€ç´¢ç ”ç©¶ç©ºç™½
        
        æŸ¥è¯¢ç­–ç•¥ï¼šæ‰¾åˆ°å¸¸ç”¨çš„æ•°æ®å­—æ®µï¼Œä½†å°šæœªä¸æŸäº›æ–¹æ³•ç»„åˆä½¿ç”¨çš„æƒ…å†µ
        """
        query = """
        // æ‰¾åˆ°ä½¿ç”¨é¢‘ç‡é«˜çš„æ•°æ®å­—æ®µ
        MATCH (d:Data)-[:FEEDS_INTO]->(ae:AnalysisEvent)
        WITH d, count(ae) as freq
        WHERE freq >= 3
        
        // æ‰¾åˆ°æ‰€æœ‰æ–¹æ³•
        MATCH (m:Method)
        
        // æ£€æŸ¥è¯¥æ•°æ®å­—æ®µæ˜¯å¦ä¸è¯¥æ–¹æ³•ç»„åˆè¿‡
        WHERE NOT EXISTS {
            MATCH (d)-[:FEEDS_INTO]->(ae2:AnalysisEvent)-[:EXECUTES]->(m)
        }
        
        RETURN 
            d.name AS data_field,
            d.description AS data_description,
            freq AS usage_frequency,
            m.name AS unused_method,
            m.description AS method_description
        ORDER BY freq DESC
        LIMIT $limit
        """
        
        return self.run_cypher(query, {"limit": limit})
    
    def retrieve_context(self, goal: str) -> str:
        """
        æ ¹æ®ç”¨æˆ·ç›®æ ‡æ£€ç´¢çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡
        
        Args:
            goal: ç”¨æˆ·ç ”ç©¶ç›®æ ‡
            
        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        # æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼šå–ç¬¬ä¸€ä¸ªå®ä½“è¯ï¼‰
        keyword = self._extract_keyword(goal)
        
        print(f"  ğŸ” æ£€ç´¢å…³é”®è¯: {keyword}")
        
        # æ£€ç´¢æœ€ä½³å®è·µ
        best_practices = self.retrieve_best_practices(keyword)
        
        # æ£€ç´¢ç ”ç©¶ç©ºç™½
        research_gaps = self.retrieve_research_gaps()
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = self._format_context(best_practices, research_gaps)
        
        return context
    
    def _extract_keyword(self, goal: str) -> str:
        """ä»ç”¨æˆ·ç›®æ ‡ä¸­æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€å•å®ç°ï¼šç§»é™¤å¸¸è§è¯æ±‡
        stop_words = ["åˆ†æ", "ç ”ç©¶", "çš„", "æŠ€æœ¯", "ç©ºç™½", "æ–¹æ³•", "å¦‚ä½•"]
        words = goal.split()
        for word in words:
            if word not in stop_words and len(word) > 1:
                return word
        return goal[:10]  # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›å‰10ä¸ªå­—ç¬¦
    
    def _format_context(self, best_practices: List[Dict], research_gaps: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬
        âœ¨ V4.1 å¢å¼ºï¼šå±•ç¤ºå®Œæ•´çš„é€»è¾‘é“¾ (Full Logic Chain)
        """
        context_parts = []
        
        # æ ¼å¼åŒ–æœ€ä½³å®è·µ
        context_parts.append("=== ğŸ“š ç›¸å…³æœ€ä½³å®è·µ (å®Œæ•´é€»è¾‘é“¾) ===\n")
        if best_practices:
            for i, practice in enumerate(best_practices, 1):
                context_parts.append(f"{i}. è®ºæ–‡: {practice.get('paper_title', 'N/A')} ({practice.get('paper_year', 'N/A')})")
                
                # âœ¨ V4.1 æ–°å¢ï¼šå±•ç¤ºå®Œæ•´çš„æ­¥éª¤é“¾
                logic_chain = practice.get('full_logic_chain', [])
                if logic_chain:
                    context_parts.append(f"   å®Œæ•´åˆ†ææµç¨‹ ({len(logic_chain)} ä¸ªæ­¥éª¤):")
                    for step in logic_chain:
                        step_id = step.get('step_id', '?')
                        context_parts.append(f"\n   ã€Step {step_id}ã€‘")
                        context_parts.append(f"     ç›®æ ‡: {step.get('objective', 'N/A')}")
                        context_parts.append(f"     æ–¹æ³•: {step.get('method', 'N/A')}")
                        
                        # æ˜¾ç¤ºé…ç½®å’ŒæŒ‡æ ‡
                        if step.get('config'):
                            context_parts.append(f"     é…ç½®: {step.get('config')}")
                        if step.get('metrics'):
                            context_parts.append(f"     æŒ‡æ ‡: {step.get('metrics')}")
                        
                        # æ˜¾ç¤ºè¾“å…¥æ•°æ®
                        inputs = step.get('inputs', [])
                        if inputs:
                            context_parts.append(f"     è¾“å…¥æ•°æ®: {', '.join(inputs)}")
                        
                        # æ˜¾ç¤ºç»“è®º
                        if step.get('conclusion'):
                            conclusion_preview = str(step.get('conclusion', ''))[:100]
                            context_parts.append(f"     ç»“è®º: {conclusion_preview}...")
                else:
                    # å…¼å®¹æ—§æ ¼å¼ï¼ˆå¦‚æœæ²¡æœ‰ full_logic_chainï¼‰
                    context_parts.append(f"   ç›®æ ‡: {practice.get('objective', 'N/A')}")
                    context_parts.append(f"   æ–¹æ³•: {practice.get('method', 'N/A')}")
                
                context_parts.append("")
        else:
            context_parts.append("  (æœªæ‰¾åˆ°ç›¸å…³æ¡ˆä¾‹)\n")
        
        # æ ¼å¼åŒ–ç ”ç©¶ç©ºç™½
        context_parts.append("=== ğŸ”¬ æ½œåœ¨ç ”ç©¶ç©ºç™½ ===\n")
        if research_gaps:
            for i, gap in enumerate(research_gaps, 1):
                context_parts.append(f"{i}. æ•°æ®å­—æ®µ: {gap.get('data_field', 'N/A')} (ä½¿ç”¨é¢‘ç‡: {gap.get('usage_frequency', 0)})")
                context_parts.append(f"   æœªä½¿ç”¨çš„æ–¹æ³•: {gap.get('unused_method', 'N/A')}")
                context_parts.append(f"   æ–¹æ³•æè¿°: {gap.get('method_description', 'N/A')}")
                context_parts.append("")
        else:
            context_parts.append("  (æœªæ‰¾åˆ°æ˜æ˜¾ç©ºç™½)\n")
        
        return "\n".join(context_parts)


# ============================================================================
# 3. LLM é…ç½®
# ============================================================================

def get_llm() -> ChatOpenAI:
    """è·å–é…ç½®å¥½çš„ Qwen LLM"""
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DASHSCOPE_API_KEY")
    
    return ChatOpenAI(
        model="qwen-max",
        openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        openai_api_key=api_key,
        temperature=0.7,
    )


# ============================================================================
# 4. å®šä¹‰èŠ‚ç‚¹ (Nodes)
# ============================================================================

# å…¨å±€ GraphTool å®ä¾‹
graph_tool = None

def initialize_graph_tool():
    """åˆå§‹åŒ–å…¨å±€ GraphTool"""
    global graph_tool
    if graph_tool is None:
        graph_tool = GraphTool()


def retrieve_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 1: çŸ¥è¯†æ£€ç´¢èŠ‚ç‚¹ (Librarian)
    
    âœ¨ V4.0 å¢å¼ºï¼šå¢åŠ æ„å›¾è½¬è¯‘ (Intent Translation)
    ä» Neo4j çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    """
    print("\n" + "="*60)
    print("ğŸ“š [æ£€ç´¢è€…] æ­£åœ¨åˆ†æç”¨æˆ·æ„å›¾å¹¶æ£€ç´¢å›¾è°±...")
    print("="*60)
    
    initialize_graph_tool()
    
    user_goal = state["user_goal"]
    print(f"  ğŸ“ ç”¨æˆ·ç›®æ ‡: {user_goal}")
    
    # âœ¨ æ­¥éª¤ 1: æ„å›¾è½¬è¯‘ - ç”¨ LLM æå–æ£€ç´¢å…³é”®è¯
    llm = get_llm()
    trans_prompt = f"""ä½ æ˜¯ä¸“åˆ©åˆ†æé¢†åŸŸçš„ä¸“å®¶ã€‚ç”¨æˆ·æƒ³è¿›è¡Œä¸“åˆ©åˆ†æï¼Œç›®æ ‡æ˜¯ï¼š"{user_goal}"ã€‚

è¯·æå– 2-3 ä¸ªæ ¸å¿ƒçš„"åˆ†ææ„å›¾å…³é”®è¯"ï¼Œç”¨äºåœ¨çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸ä¼¼çš„åˆ†æä»»åŠ¡ã€‚
è¿™äº›å…³é”®è¯åº”è¯¥æ˜¯æ–¹æ³•è®ºæœ¯è¯­ã€åˆ†æç›®æ ‡æˆ–æŠ€æœ¯é¢†åŸŸã€‚

ç¤ºä¾‹ï¼š
- ç”¨æˆ·ï¼š"åˆ†æå›ºæ€ç”µæ± æŠ€æœ¯ç©ºç™½" -> å…³é”®è¯ï¼šæŠ€æœ¯ç©ºç™½, è¯†åˆ«, èšç±»
- ç”¨æˆ·ï¼š"ç«äº‰å¯¹æ‰‹åˆ†æ" -> å…³é”®è¯ï¼šç«äº‰å¯¹æ‰‹, å¸‚åœºä»½é¢, HHI
- ç”¨æˆ·ï¼š"æŠ€æœ¯æ¼”åŒ–è·¯å¾„" -> å…³é”®è¯ï¼šæŠ€æœ¯æ¼”åŒ–, è·¯å¾„åˆ†æ, å¼•ç”¨ç½‘ç»œ

åªè¿”å›å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

    keywords_response = llm.invoke(trans_prompt)
    keywords_str = keywords_response.content.strip()
    keywords = [k.strip() for k in keywords_str.split(",")]
    
    print(f"  ğŸ§  æ„å›¾å…³é”®è¯: {keywords}")
    
    # âœ¨ æ­¥éª¤ 2: å¤šå…³é”®è¯æ£€ç´¢ - å¾ªç¯æ£€ç´¢æ‰€æœ‰å…³é”®è¯
    all_practices = []
    for kw in keywords:
        print(f"     ğŸ” æ£€ç´¢å…³é”®è¯: {kw}")
        res = graph_tool.retrieve_best_practices(kw, limit=2)
        all_practices.extend(res)
    
    # å»é‡ï¼ˆåŸºäºè®ºæ–‡æ ‡é¢˜ï¼‰
    unique_practices = {p['paper_title']: p for p in all_practices}.values()
    unique_practices = list(unique_practices)
    
    print(f"  âœ“ æ£€ç´¢åˆ° {len(unique_practices)} ä¸ªç‹¬ç‰¹æ¡ˆä¾‹")
    
    # âœ¨ æ­¥éª¤ 3: æ£€ç´¢ç ”ç©¶ç©ºç™½
    gaps = graph_tool.retrieve_research_gaps()
    
    # âœ¨ æ­¥éª¤ 4: æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
    context = graph_tool._format_context(unique_practices, gaps)
    
    print(f"  âœ“ æ£€ç´¢å®Œæˆï¼Œæ„å»ºäº† {len(context)} å­—ç¬¦çš„ä¸Šä¸‹æ–‡")
    
    return {"graph_context": context}


def generate_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 2: æ–¹æ¡ˆç”ŸæˆèŠ‚ç‚¹ (Strategist)
    
    âœ¨ V4.0 å¢å¼ºï¼šå¼ºåŒ–è·¨åŸŸè¿ç§» (Transfer Learning) èƒ½åŠ›
    åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç ”ç©¶æ–¹æ¡ˆ
    """
    print("\n" + "="*60)
    print("ğŸ’¡ [æˆ˜ç•¥å®¶] æ­£åœ¨ç”Ÿæˆç ”ç©¶æ–¹æ¡ˆ...")
    print("="*60)
    
    user_goal = state["user_goal"]
    graph_context = state["graph_context"]
    
    # âœ¨ æ„å»ºè·¨åŸŸè¿ç§» Prompt
    prompt = f"""ä½ æ˜¯ä¸€ä½ç²¾é€š"è·¨åŸŸåˆ›æ–°"çš„ä¸“åˆ©åˆ†ææˆ˜ç•¥å®¶ã€‚ä½ çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯**ç±»æ¯”æ¨ç†**å’Œ**æ–¹æ³•è®ºè¿ç§»**ã€‚

**ç”¨æˆ·ç›®æ ‡:** 
{user_goal}

**å›¾è°±è®°å¿† (å†å²ä¸Šçš„æˆåŠŸæ¡ˆä¾‹):**
{graph_context}

**ä»»åŠ¡:** 
è¯·æ¨¡ä»¿å›¾è°±ä¸­çš„æˆåŠŸæ–¹æ³•è®ºï¼ˆBest Practicesï¼‰ï¼Œä¸ºç”¨æˆ·çš„ç›®æ ‡é¢†åŸŸè®¾è®¡ä¸€ä¸ªè¯¦ç»†çš„ç ”ç©¶æ–¹æ¡ˆã€‚

**ğŸ”‘ å…³é”®æ€è€ƒé€»è¾‘:**
1. **è§‚å¯Ÿæ¨¡å¼**: å›¾è°±ä¸­åˆ«äººæ˜¯å¦‚ä½•è§£å†³ç±»ä¼¼é—®é¢˜çš„ï¼Ÿ
   - ä¾‹å¦‚ï¼šåˆ«äººç”¨ TCT ç®—æŠ€æœ¯å‘¨æœŸï¼Œç”¨ HHI ç®—å¸‚åœºå„æ–­ï¼Œç”¨èšç±»è¯†åˆ«ç©ºç™½
2. **è·¨åŸŸè¿ç§»**: å°†è¿™äº›æ–¹æ³•**ç§»æ¤**åˆ°ç”¨æˆ·çš„é—®é¢˜ä¸Š
   - å³ä½¿å›¾è°±ä¸­æ²¡æœ‰å…³äº"{user_goal}"çš„ç›´æ¥æ¡ˆä¾‹ï¼Œä½ ä¹Ÿè¦ä»å…¶ä»–é¢†åŸŸï¼ˆå¦‚é€šä¿¡ã€ç”Ÿç‰©ã€èƒ½æºï¼‰è¿ç§»æ–¹æ³•
3. **å…·ä½“åŒ–**: å¿…é¡»æä¾›**å¯æ‰§è¡Œçš„é…ç½®**å’Œ**å¯æµ‹é‡çš„æŒ‡æ ‡**
   - å‚è€ƒå›¾è°±ä¸­çš„ config å’Œ metrics å­—æ®µ
   - ä¾‹å¦‚ï¼šconfig: {{"library": "Gensim", "params": "min_count=5"}}, metrics: "TCT < 5å¹´"

**âš ï¸ é‡è¦çº¦æŸ:**
- å³ä½¿ç”¨æˆ·çš„ç›®æ ‡é¢†åŸŸåœ¨å›¾è°±ä¸­æ²¡æœ‰ç›´æ¥å‡ºç°ï¼Œä½ ä¹Ÿå¿…é¡»è¿›è¡Œç±»æ¯”
- å¿…é¡»åŒ…å«å…·ä½“çš„æ‰§è¡Œé…ç½®ï¼ˆLibrary, Paramsï¼‰å’Œé¢„æœŸæŒ‡æ ‡ï¼ˆMetricsï¼‰
- æ–¹æ¡ˆå¿…é¡»æ˜¯å¯æ“ä½œçš„ï¼Œä¸èƒ½åªæœ‰æŠ½è±¡æè¿°

**è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼ JSON):**
{{
  "hypothesis": "æ ¸å¿ƒç ”ç©¶å‡è®¾",
  "reference_case": "å‚è€ƒäº†å›¾è°±ä¸­çš„å“ªç¯‡è®ºæ–‡/å“ªä¸ªæ–¹æ³•",
  "method_plan": {{
    "method_name": "å…·ä½“æ–¹æ³•åç§°",
    "config": {{
      "library": "ä½¿ç”¨çš„å·¥å…·åº“ï¼ˆå¦‚ Gensim, NetworkX, Scikit-learnï¼‰",
      "params": "å…³é”®å‚æ•°é…ç½®"
    }},
    "target_metric": "é¢„æœŸæŒ‡æ ‡ï¼ˆå¦‚ TCT < 5å¹´, HHI > 0.6, èšç±»æ•° = 5-8ï¼‰"
  }},
  "data_sources": ["æ•°æ®å­—æ®µ1", "æ•°æ®å­—æ®µ2"],
  "reasoning": "ä¸ºä»€ä¹ˆè¿™ä¸ªæ—§æ–¹æ³•é€‚ç”¨äºè¿™ä¸ªæ–°é—®é¢˜ï¼Ÿï¼ˆç±»æ¯”æ¨ç†è¿‡ç¨‹ï¼‰",
  "innovation_points": ["åˆ›æ–°ç‚¹1", "åˆ›æ–°ç‚¹2"]
}}

è¯·ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""

    # è°ƒç”¨ LLM
    llm = get_llm()
    response = llm.invoke(prompt)
    
    # è§£æ JSON
    try:
        # æå– JSON å†…å®¹ï¼ˆå¤„ç†å¯èƒ½çš„ markdown ä»£ç å—ï¼‰
        content = response.content.strip()
        if content.startswith("```"):
            # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
            content = content.strip()
        
        idea_json = json.loads(content)
        print("  âœ“ æ–¹æ¡ˆç”ŸæˆæˆåŠŸ")
        
    except json.JSONDecodeError as e:
        print(f"  âš  JSON è§£æå¤±è´¥: {e}")
        print(f"  åŸå§‹å“åº”: {response.content[:200]}...")
        idea_json = {
            "error": "JSON è§£æå¤±è´¥",
            "raw_response": response.content
        }
    
    return {"generated_idea": idea_json}


def critique_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 3: åæ€/è¯„ä»·èŠ‚ç‚¹ (Critic)
    
    âœ¨ V4.0 æ–°å¢ï¼šæ£€æŸ¥ç”Ÿæˆæ–¹æ¡ˆçš„è´¨é‡
    ç¡®ä¿æ–¹æ¡ˆåŒ…å«å…·ä½“çš„ config å’Œ metrics
    """
    print("\n" + "="*60)
    print("ğŸ” [è¯„ä»·è€…] æ­£åœ¨æ£€æŸ¥æ–¹æ¡ˆè´¨é‡...")
    print("="*60)
    
    generated_idea = state["generated_idea"]
    iteration_count = state.get("iteration_count", 0)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if "error" in generated_idea:
        print("  âš ï¸ æ–¹æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡è´¨é‡æ£€æŸ¥")
        return {
            "critique": "æ–¹æ¡ˆç”Ÿæˆå¤±è´¥",
            "quality_passed": False,
            "iteration_count": iteration_count + 1
        }
    
    # è´¨é‡æ£€æŸ¥æ ‡å‡†
    checks = {
        "æœ‰ method_plan": "method_plan" in generated_idea,
        "æœ‰ config": "method_plan" in generated_idea and "config" in generated_idea.get("method_plan", {}),
        "æœ‰ library": "method_plan" in generated_idea and "library" in generated_idea.get("method_plan", {}).get("config", {}),
        "æœ‰ target_metric": "method_plan" in generated_idea and "target_metric" in generated_idea.get("method_plan", {}),
        "æœ‰ reasoning": "reasoning" in generated_idea and len(generated_idea.get("reasoning", "")) > 20,
    }
    
    # ç»Ÿè®¡é€šè¿‡çš„æ£€æŸ¥é¡¹
    passed_checks = sum(checks.values())
    total_checks = len(checks)
    
    print(f"  ğŸ“Š è´¨é‡æ£€æŸ¥: {passed_checks}/{total_checks} é¡¹é€šè¿‡")
    for check_name, passed in checks.items():
        status = "âœ“" if passed else "âœ—"
        print(f"     {status} {check_name}")
    
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡
    quality_passed = passed_checks >= 4  # è‡³å°‘é€šè¿‡ 4/5 é¡¹
    
    if quality_passed:
        critique = f"è´¨é‡æ£€æŸ¥é€šè¿‡ ({passed_checks}/{total_checks})"
        print(f"  âœ… {critique}")
    else:
        critique = f"è´¨é‡ä¸è¶³ ({passed_checks}/{total_checks})ï¼Œç¼ºå°‘å…·ä½“é…ç½®æˆ–æŒ‡æ ‡"
        print(f"  âš ï¸ {critique}")
    
    return {
        "critique": critique,
        "quality_passed": quality_passed,
        "iteration_count": iteration_count + 1
    }


def should_regenerate(state: AgentState) -> str:
    """
    æ¡ä»¶è¾¹ï¼šå†³å®šæ˜¯å¦éœ€è¦é‡æ–°ç”Ÿæˆ
    
    è¿”å›:
        - "end": è´¨é‡é€šè¿‡æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç»“æŸæµç¨‹
        - "regenerate": è´¨é‡ä¸é€šè¿‡ä¸”æœªè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé‡æ–°ç”Ÿæˆ
    """
    quality_passed = state.get("quality_passed", False)
    iteration_count = state.get("iteration_count", 0)
    max_iterations = 2  # æœ€å¤šé‡è¯• 2 æ¬¡
    
    if quality_passed:
        print("  âœ… è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œæµç¨‹ç»“æŸ")
        return "end"
    elif iteration_count >= max_iterations:
        print(f"  âš ï¸ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œæµç¨‹ç»“æŸ")
        return "end"
    else:
        print(f"  ğŸ”„ è´¨é‡ä¸è¶³ï¼Œé‡æ–°ç”Ÿæˆ (ç¬¬ {iteration_count + 1} æ¬¡å°è¯•)")
        return "regenerate"


# ============================================================================
# 5. æ„å»º LangGraph å·¥ä½œæµ
# ============================================================================

def build_graph() -> Any:
    """
    æ„å»º LangGraph å·¥ä½œæµ
    
    âœ¨ V4.0 å¢å¼ºæµç¨‹: 
    START -> librarian -> strategist -> critic -> [åˆ¤æ–­] -> END æˆ– regenerate
    """
    print("\nğŸ”§ æ„å»º LangGraph å·¥ä½œæµ...")
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("librarian", retrieve_node)      # æ£€ç´¢è€…
    workflow.add_node("strategist", generate_node)     # æˆ˜ç•¥å®¶
    workflow.add_node("critic", critique_node)         # âœ¨ è¯„ä»·è€…
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("librarian")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("librarian", "strategist")
    workflow.add_edge("strategist", "critic")
    
    # âœ¨ æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®è´¨é‡æ£€æŸ¥ç»“æœå†³å®šæ˜¯å¦é‡æ–°ç”Ÿæˆ
    workflow.add_conditional_edges(
        "critic",
        should_regenerate,
        {
            "end": END,
            "regenerate": "strategist"  # å›åˆ°ç”ŸæˆèŠ‚ç‚¹
        }
    )
    
    print("  âœ“ å·¥ä½œæµæ„å»ºå®Œæˆ")
    print("  æµç¨‹: START -> librarian -> strategist -> critic -> [è´¨é‡æ£€æŸ¥] -> END/regenerate")
    
    # ç¼–è¯‘å›¾
    return workflow.compile()


# ============================================================================
# 6. ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ Patent-DeepScientist - Idea æå‡ºæ¨¡å—")
    print("="*60)
    
    # æ„å»ºå·¥ä½œæµ
    app = build_graph()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_goals = [
        "åˆ†æå›ºæ€ç”µæ± çš„æŠ€æœ¯ç©ºç™½",
        "ç ”ç©¶äººå·¥æ™ºèƒ½åœ¨ä¸“åˆ©åˆ†æä¸­çš„åº”ç”¨",
        "æ¢ç´¢åŒºå—é“¾æŠ€æœ¯çš„ä¸“åˆ©å¸ƒå±€ç­–ç•¥"
    ]
    
    # æ‰§è¡Œç¬¬ä¸€ä¸ªæµ‹è¯•
    user_goal = test_goals[0]
    print(f"\nğŸ¯ æµ‹è¯•ç›®æ ‡: {user_goal}")
    
    try:
        # è°ƒç”¨å·¥ä½œæµ
        result = app.invoke({
            "user_goal": user_goal,
            "graph_context": "",
            "generated_idea": {},
            "critique": "",
            "quality_passed": False,
            "iteration_count": 0
        })
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ“Š æœ€ç»ˆç”Ÿæˆçš„ç ”ç©¶æ–¹æ¡ˆ:")
        print("="*60)
        print(json.dumps(result['generated_idea'], indent=2, ensure_ascii=False))
        
        # ä¿å­˜ç»“æœ
        output_file = "strategist_output.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        if graph_tool:
            graph_tool.close()
            print("\nâœ“ Neo4j è¿æ¥å·²å…³é—­")
    
    print("\n" + "="*60)
    print("âœ… æ‰§è¡Œå®Œæˆ")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()

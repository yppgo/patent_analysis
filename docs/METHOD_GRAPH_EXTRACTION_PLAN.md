# æ–¹æ³•å›¾è°±é‡æ„æå–æ–¹æ¡ˆ

## ğŸ“‹ ç›®æ ‡

ä»50ç¯‡ä¸“åˆ©åˆ†ææ–‡çŒ®ä¸­æå–**å‡è®¾éªŒè¯æ‰€éœ€çš„æ–¹æ³•çŸ¥è¯†**ï¼Œæ„å»ºé€‚é…å› æœæ¨æ–­ä»»åŠ¡çš„æ–¹æ³•å›¾è°±ã€‚

---

## ğŸ“Š è¾“å…¥æ•°æ®

### è¾“å…¥1ï¼šæ–‡çŒ®PDFæ–‡ä»¶
**ä½ç½®**ï¼š`batch_50_results/`
**æ•°é‡**ï¼š50ç¯‡
**æ ¼å¼**ï¼šPDFæ–‡ä»¶ + å·²æœ‰çš„JSONåˆ†æç»“æœ

### è¾“å…¥2ï¼šå› æœå›¾è°±å˜é‡
**ä½ç½®**ï¼š`sandbox/static/data/causal_ontology_extracted.json`
**å†…å®¹**ï¼š30ä¸ªå˜é‡å®šä¹‰
**ç¤ºä¾‹**ï¼š
```json
{
  "id": "V09_tech_diversity",
  "label": "æŠ€æœ¯è·¨ç•Œåº¦",
  "category": "mediator",
  "definition": "ä¸“åˆ©æ¶‰åŠçš„IPCåˆ†ç±»çš„å¤šæ ·æ€§",
  "measurement": {
    "metric": "ipc_entropy",
    "formula": "Shannon Entropy = -SUM(p_i * log(p_i))",
    "unit": "ç†µå€¼"
  }
}
```

---

## ğŸ¯ è¾“å‡ºæ•°æ®ç»“æ„

### è¾“å‡º1ï¼šå˜é‡æµ‹é‡æ–¹æ³•ï¼ˆJSONï¼‰

**æ–‡ä»¶**ï¼š`outputs/variable_measurement_methods.json`

**ç»“æ„**ï¼š
```json
{
  "meta": {
    "extraction_date": "2026-01-19",
    "source_papers": 50,
    "total_variables": 30,
    "extracted_methods": 25
  },
  "measurements": [
    {
      "variable_id": "V09_tech_diversity",
      "variable_name": "æŠ€æœ¯è·¨ç•Œåº¦",
      "measurement_methods": [
        {
          "method_id": "M001",
          "method_name": "Shannon Entropy",
          "usage_frequency": 0.60,
          "recommendation_level": "æ¨è",
          "recommendation_reason": "æœ€å¸¸ç”¨æ–¹æ³•ï¼Œç†è®ºåŸºç¡€æ‰å®ï¼Œè€ƒè™‘åˆ†å¸ƒå‡åŒ€æ€§",
          "description": "è®¡ç®—IPCåˆ†ç±»çš„ä¿¡æ¯ç†µæ¥è¡¡é‡æŠ€æœ¯å¤šæ ·æ€§",
          "formula": "-SUM(p_i * log(p_i))",
          "formula_explanation": "p_iæ˜¯ç¬¬iä¸ªIPCç±»åˆ«çš„ä¸“åˆ©å æ¯”",
          "data_requirements": [
            {
              "data_type": "IPCåˆ†ç±»å·",
              "data_format": "å­—ç¬¦ä¸²ï¼Œåˆ†å·åˆ†éš”",
              "example": "G06F17/30; H04L29/06",
              "excel_column_candidates": [
                "IPCä¸»åˆ†ç±»å·",
                "IPCåˆ†ç±»å·",
                "æŠ€æœ¯åˆ†ç±»"
              ]
            }
          ],
          "calculation_steps": [
            "1. æå–IPCå¤§ç±»ï¼ˆå‰4ä½å­—ç¬¦ï¼‰",
            "2. ç»Ÿè®¡æ¯ä¸ªå¤§ç±»çš„ä¸“åˆ©æ•°é‡",
            "3. è®¡ç®—æ¯ä¸ªå¤§ç±»çš„å æ¯” p_i",
            "4. åº”ç”¨Shannonç†µå…¬å¼ï¼š-SUM(p_i * log2(p_i))"
          ],
          "python_implementation": {
            "library": "scipy.stats",
            "function": "entropy",
            "code_snippet": "from scipy.stats import entropy\nimport pandas as pd\n\ndef calculate_ipc_entropy(ipc_string):\n    if pd.isna(ipc_string):\n        return 0\n    ipc_classes = [ipc[:4] for ipc in str(ipc_string).split(';')]\n    class_counts = pd.Series(ipc_classes).value_counts()\n    probabilities = class_counts / class_counts.sum()\n    return entropy(probabilities, base=2)"
          },
          "evidence": {
            "paper_count": 14,
            "key_papers": [
              "Fleming (2001) - Recombinant uncertainty in technological search",
              "Verhoeven et al. (2016) - Measuring technological novelty"
            ],
            "citation_context": "æŠ€æœ¯å¤šæ ·æ€§é€šå¸¸ä½¿ç”¨IPCåˆ†ç±»çš„Shannonç†µæ¥æµ‹é‡"
          },
          "validation": {
            "typical_range": [0, 3.5],
            "interpretation": "å€¼è¶Šå¤§è¡¨ç¤ºæŠ€æœ¯è¶Šå¤šæ ·åŒ–",
            "quality_check": "æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜å€¼ï¼ˆ>4ï¼‰æˆ–è´Ÿå€¼"
          }
        },
        {
          "method_id": "M002",
          "method_name": "IPCç±»åˆ«è®¡æ•°",
          "usage_frequency": 0.30,
          "recommendation_level": "å¤‡é€‰",
          "recommendation_reason": "ç®€å•å¿«é€Ÿï¼Œé€‚åˆåˆæ­¥åˆ†ææˆ–æ•°æ®ä¸å®Œæ•´æ—¶ä½¿ç”¨",
          "description": "ç®€å•è®¡æ•°ä¸“åˆ©æ¶‰åŠçš„IPCå¤§ç±»æ•°é‡",
          "formula": "COUNT(DISTINCT IPC_class)",
          "data_requirements": [
            {
              "data_type": "IPCåˆ†ç±»å·",
              "data_format": "å­—ç¬¦ä¸²ï¼Œåˆ†å·åˆ†éš”",
              "example": "G06F17/30; H04L29/06"
            }
          ],
          "calculation_steps": [
            "1. æå–IPCå¤§ç±»ï¼ˆå‰4ä½ï¼‰",
            "2. å»é‡",
            "3. è®¡æ•°"
          ],
          "python_implementation": {
            "code_snippet": "def count_ipc_classes(ipc_string):\n    if pd.isna(ipc_string):\n        return 0\n    ipc_classes = set([ipc[:4] for ipc in str(ipc_string).split(';')])\n    return len(ipc_classes)"
          },
          "evidence": {
            "paper_count": 8,
            "key_papers": ["Lerner (1994)"]
          }
        },
        {
          "method_id": "M002b",
          "method_name": "HerfindahlæŒ‡æ•° (HHI)",
          "usage_frequency": 0.10,
          "recommendation_level": "å¯é€‰",
          "recommendation_reason": "ç»æµå­¦è§†è§’ï¼Œä¸Shannonç†µé«˜åº¦ç›¸å…³",
          "description": "ä½¿ç”¨HerfindahlæŒ‡æ•°è¡¡é‡æŠ€æœ¯é›†ä¸­åº¦ï¼ˆ1-HHIå³ä¸ºå¤šæ ·æ€§ï¼‰",
          "formula": "1 - SUM(p_i^2)",
          "data_requirements": [
            {
              "data_type": "IPCåˆ†ç±»å·",
              "data_format": "å­—ç¬¦ä¸²ï¼Œåˆ†å·åˆ†éš”"
            }
          ],
          "python_implementation": {
            "code_snippet": "def calculate_hhi_diversity(ipc_string):\n    if pd.isna(ipc_string):\n        return 0\n    ipc_classes = [ipc[:4] for ipc in str(ipc_string).split(';')]\n    class_counts = pd.Series(ipc_classes).value_counts()\n    probabilities = class_counts / class_counts.sum()\n    hhi = (probabilities ** 2).sum()\n    return 1 - hhi  # å¤šæ ·æ€§ = 1 - é›†ä¸­åº¦"
          },
          "evidence": {
            "paper_count": 5,
            "key_papers": ["Jaffe (1986)"]
          }
        }
      ],
      "default_method": "M001",
      "method_selection_logic": {
        "default": "M001",
        "if_data_incomplete": "M002",
        "if_quick_analysis": "M002",
        "if_economic_perspective": "M002b"
      },
      "method_comparison": {
        "correlation_matrix": {
          "M001_vs_M002": 0.85,
          "M001_vs_M002b": 0.92,
          "M002_vs_M002b": 0.78
        },
        "pros_cons": {
          "M001": {
            "pros": ["ç†è®ºä¸¥è°¨", "è€ƒè™‘åˆ†å¸ƒ", "æ–‡çŒ®æ”¯æŒå¤š"],
            "cons": ["è®¡ç®—å¤æ‚", "éœ€è¦å®Œæ•´æ•°æ®"]
          },
          "M002": {
            "pros": ["ç®€å•ç›´è§‚", "è®¡ç®—å¿«é€Ÿ", "æ•°æ®è¦æ±‚ä½"],
            "cons": ["ä¸è€ƒè™‘åˆ†å¸ƒ", "ä¿¡æ¯æŸå¤±"]
          },
          "M002b": {
            "pros": ["ç»æµå­¦å¸¸ç”¨", "ä¸ç†µå€¼ç›¸å…³"],
            "cons": ["ä½¿ç”¨è¾ƒå°‘", "è§£é‡Šæ€§ä¸å¦‚ç†µ"]
          }
        }
      }
    },
    {
      "variable_id": "V16_tech_impact",
      "variable_name": "æŠ€æœ¯å½±å“åŠ›",
      "measurement_methods": [
        {
          "method_id": "M003",
          "method_name": "å‰å‘å¼•ç”¨è®¡æ•°",
          "description": "ç»Ÿè®¡ä¸“åˆ©è¢«åç»­ä¸“åˆ©å¼•ç”¨çš„æ¬¡æ•°",
          "formula": "COUNT(forward_citations)",
          "data_requirements": [
            {
              "data_type": "è¢«å¼•ç”¨ä¸“åˆ©æ•°é‡",
              "data_format": "æ•´æ•°",
              "example": "15",
              "excel_column_candidates": [
                "è¢«å¼•ç”¨ä¸“åˆ©æ•°é‡",
                "å‰å‘å¼•ç”¨",
                "å¼•ç”¨æ¬¡æ•°",
                "forward_citations"
              ]
            }
          ],
          "calculation_steps": [
            "1. ç›´æ¥è¯»å–è¢«å¼•ç”¨æ•°é‡å­—æ®µ",
            "2. å¤„ç†ç¼ºå¤±å€¼ï¼ˆå¡«å……ä¸º0ï¼‰",
            "3. å¯é€‰ï¼šæŒ‰ç”³è¯·å¹´ä»½æ ‡å‡†åŒ–"
          ],
          "python_implementation": {
            "code_snippet": "def get_forward_citations(df):\n    return df['è¢«å¼•ç”¨ä¸“åˆ©æ•°é‡'].fillna(0).astype(int)"
          },
          "evidence": {
            "paper_count": 35,
            "key_papers": [
              "Trajtenberg (1990) - A penny for your quotes",
              "Hall et al. (2005) - Market value and patent citations"
            ]
          },
          "validation": {
            "typical_range": [0, 100],
            "interpretation": "å€¼è¶Šå¤§è¡¨ç¤ºæŠ€æœ¯å½±å“åŠ›è¶Šå¤§",
            "quality_check": "æ£€æŸ¥å¼‚å¸¸é«˜å€¼ï¼ˆ>500ï¼‰"
          }
        }
      ]
    }
  ]
}
```

### è¾“å‡º2ï¼šç»Ÿè®¡åˆ†ææ–¹æ³•ï¼ˆJSONï¼‰

**æ–‡ä»¶**ï¼š`outputs/statistical_analysis_methods.json`

**ç»“æ„**ï¼š
```json
{
  "meta": {
    "extraction_date": "2026-01-19",
    "source_papers": 50,
    "total_methods": 15
  },
  "analysis_methods": [
    {
      "method_id": "A001",
      "method_name": "OLSå›å½’åˆ†æ",
      "method_type": "regression",
      "description": "æ™®é€šæœ€å°äºŒä¹˜å›å½’ï¼Œç”¨äºæ£€éªŒè‡ªå˜é‡å¯¹å› å˜é‡çš„å½±å“",
      "applicable_scenarios": [
        "å•è·³å‡è®¾éªŒè¯ï¼ˆX â†’ Yï¼‰",
        "æ§åˆ¶å˜é‡åˆ†æ",
        "çº¿æ€§å…³ç³»æ£€éªŒ"
      ],
      "formula": "Y = Î²0 + Î²1*X + Î²2*Control1 + Î²3*Control2 + Îµ",
      "assumptions": [
        "çº¿æ€§å…³ç³»",
        "æ®‹å·®æ­£æ€åˆ†å¸ƒ",
        "åŒæ–¹å·®æ€§",
        "æ— å¤šé‡å…±çº¿æ€§"
      ],
      "python_implementation": {
        "library": "statsmodels",
        "code_template": "import statsmodels.api as sm\n\n# å‡†å¤‡æ•°æ®\nX = df[['independent_var', 'control1', 'control2']]\nX = sm.add_constant(X)  # æ·»åŠ å¸¸æ•°é¡¹\ny = df['dependent_var']\n\n# æ‹Ÿåˆæ¨¡å‹\nmodel = sm.OLS(y, X).fit()\n\n# æŸ¥çœ‹ç»“æœ\nprint(model.summary())\n\n# æå–å…³é”®æŒ‡æ ‡\ncoef = model.params['independent_var']  # å›å½’ç³»æ•°\np_value = model.pvalues['independent_var']  # på€¼\nr_squared = model.rsquared  # RÂ²\n\n# åˆ¤æ–­å‡è®¾æ˜¯å¦æˆç«‹\nif p_value < 0.05 and coef > 0:\n    print('å‡è®¾æˆç«‹ï¼šXå¯¹Yæœ‰æ˜¾è‘—æ­£å‘å½±å“')"
      },
      "output_interpretation": {
        "coefficient": "å›å½’ç³»æ•°ï¼Œè¡¨ç¤ºXæ¯å¢åŠ 1ä¸ªå•ä½ï¼ŒYå¹³å‡å˜åŒ–å¤šå°‘",
        "p_value": "æ˜¾è‘—æ€§æ°´å¹³ï¼Œ<0.05è¡¨ç¤ºæ˜¾è‘—ï¼Œ<0.01è¡¨ç¤ºé«˜åº¦æ˜¾è‘—",
        "r_squared": "è§£é‡ŠåŠ›ï¼Œè¡¨ç¤ºXè§£é‡Šäº†Yå¤šå°‘ç™¾åˆ†æ¯”çš„å˜å¼‚",
        "confidence_interval": "95%ç½®ä¿¡åŒºé—´ï¼Œä¸åŒ…å«0åˆ™æ˜¾è‘—"
      },
      "evidence": {
        "paper_count": 42,
        "usage_frequency": "84%",
        "key_papers": [
          "å¤§éƒ¨åˆ†å®è¯ç ”ç©¶è®ºæ–‡"
        ]
      },
      "common_controls": [
        "ç”³è¯·å¹´ä»½ï¼ˆæ§åˆ¶æ—¶é—´è¶‹åŠ¿ï¼‰",
        "ä¼ä¸šè§„æ¨¡ï¼ˆæ§åˆ¶è§„æ¨¡æ•ˆåº”ï¼‰",
        "æŠ€æœ¯é¢†åŸŸï¼ˆæ§åˆ¶è¡Œä¸šå·®å¼‚ï¼‰"
      ]
    },
    {
      "method_id": "A002",
      "method_name": "ä¸­ä»‹æ•ˆåº”æ£€éªŒ",
      "method_type": "mediation",
      "description": "æ£€éªŒXé€šè¿‡Må½±å“Yçš„ä¸­ä»‹æœºåˆ¶",
      "applicable_scenarios": [
        "å¤šè·³å‡è®¾éªŒè¯ï¼ˆX â†’ M â†’ Yï¼‰",
        "æœºåˆ¶æ¢ç´¢",
        "è·¯å¾„åˆ†æ"
      ],
      "steps": [
        "Step 1: æ£€éªŒ X â†’ Y çš„æ€»æ•ˆåº”ï¼ˆcè·¯å¾„ï¼‰",
        "Step 2: æ£€éªŒ X â†’ M çš„æ•ˆåº”ï¼ˆaè·¯å¾„ï¼‰",
        "Step 3: æ£€éªŒ M â†’ Y çš„æ•ˆåº”ï¼ˆbè·¯å¾„ï¼‰ï¼Œæ§åˆ¶X",
        "Step 4: æ£€éªŒ X â†’ Y çš„ç›´æ¥æ•ˆåº”ï¼ˆc'è·¯å¾„ï¼‰ï¼Œæ§åˆ¶M",
        "Step 5: è®¡ç®—ä¸­ä»‹æ•ˆåº” = a Ã— b"
      ],
      "python_implementation": {
        "library": "statsmodels + custom",
        "code_template": "import statsmodels.api as sm\nimport numpy as np\n\n# Step 1: æ€»æ•ˆåº” (c)\nX = sm.add_constant(df['X'])\ny = df['Y']\nmodel_c = sm.OLS(y, X).fit()\nc = model_c.params['X']\n\n# Step 2: aè·¯å¾„ (X â†’ M)\nm = df['M']\nmodel_a = sm.OLS(m, X).fit()\na = model_a.params['X']\n\n# Step 3: bè·¯å¾„ (M â†’ Y, æ§åˆ¶X)\nX_M = sm.add_constant(df[['X', 'M']])\nmodel_b = sm.OLS(y, X_M).fit()\nb = model_b.params['M']\nc_prime = model_b.params['X']  # ç›´æ¥æ•ˆåº”\n\n# Step 4: è®¡ç®—ä¸­ä»‹æ•ˆåº”\nindirect_effect = a * b\nmediation_ratio = indirect_effect / c\n\nprint(f'æ€»æ•ˆåº”: {c:.3f}')\nprint(f'ç›´æ¥æ•ˆåº”: {c_prime:.3f}')\nprint(f'ä¸­ä»‹æ•ˆåº”: {indirect_effect:.3f}')\nprint(f'ä¸­ä»‹æ¯”ä¾‹: {mediation_ratio:.1%}')\n\n# Sobelæ£€éªŒ\nse_indirect = np.sqrt(b**2 * model_a.bse['X']**2 + a**2 * model_b.bse['M']**2)\nz_score = indirect_effect / se_indirect\np_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n\nif p_value < 0.05:\n    print('ä¸­ä»‹æ•ˆåº”æ˜¾è‘—')"
      },
      "evidence": {
        "paper_count": 12,
        "key_papers": [
          "Baron & Kenny (1986) - The moderator-mediator variable distinction"
        ]
      }
    },
    {
      "method_id": "A003",
      "method_name": "è°ƒèŠ‚æ•ˆåº”æ£€éªŒ",
      "method_type": "moderation",
      "description": "æ£€éªŒMå¦‚ä½•è°ƒèŠ‚Xå¯¹Yçš„å½±å“",
      "applicable_scenarios": [
        "è¾¹ç•Œæ¡ä»¶æ¢ç´¢",
        "äº¤äº’æ•ˆåº”æ£€éªŒ",
        "æƒ…å¢ƒå› ç´ åˆ†æ"
      ],
      "formula": "Y = Î²0 + Î²1*X + Î²2*M + Î²3*X*M + Îµ",
      "python_implementation": {
        "code_template": "import statsmodels.api as sm\n\n# åˆ›å»ºäº¤äº’é¡¹\ndf['X_M_interaction'] = df['X'] * df['M']\n\n# å›å½’åˆ†æ\nX = sm.add_constant(df[['X', 'M', 'X_M_interaction']])\ny = df['Y']\nmodel = sm.OLS(y, X).fit()\n\n# æŸ¥çœ‹äº¤äº’é¡¹ç³»æ•°\ninteraction_coef = model.params['X_M_interaction']\ninteraction_p = model.pvalues['X_M_interaction']\n\nif interaction_p < 0.05:\n    print(f'è°ƒèŠ‚æ•ˆåº”æ˜¾è‘—: Î²3 = {interaction_coef:.3f}, p = {interaction_p:.3f}')\n    \n    # ç®€å•æ–œç‡åˆ†æ\n    m_low = df['M'].quantile(0.25)\n    m_high = df['M'].quantile(0.75)\n    \n    slope_low = model.params['X'] + model.params['X_M_interaction'] * m_low\n    slope_high = model.params['X'] + model.params['X_M_interaction'] * m_high\n    \n    print(f'Mä½æ—¶ï¼ŒXå¯¹Yçš„å½±å“: {slope_low:.3f}')\n    print(f'Mé«˜æ—¶ï¼ŒXå¯¹Yçš„å½±å“: {slope_high:.3f}')"
      },
      "evidence": {
        "paper_count": 8,
        "key_papers": [
          "Aiken & West (1991) - Multiple regression"
        ]
      }
    }
  ]
}
```

### è¾“å‡º3ï¼šNeo4jå¯¼å…¥è„šæœ¬

**æ–‡ä»¶**ï¼š`scripts/import_method_graph_v2.py`

**åŠŸèƒ½**ï¼šå°†JSONæ•°æ®å¯¼å…¥Neo4jï¼Œæ„å»ºæ–°çš„å›¾è°±ç»“æ„

**å›¾è°±ç»“æ„**ï¼š
```
Variable --[HAS_MEASUREMENT]--> MeasurementMethod --[REQUIRES_DATA]--> DataField
Variable --[CAN_BE_ANALYZED_BY]--> AnalysisMethod
Hypothesis --[USES_VARIABLE]--> Variable
Hypothesis --[REQUIRES_ANALYSIS]--> AnalysisMethod
```

---

## ğŸ”§ æå–æµç¨‹

### Step 1ï¼šè®¾è®¡æå–Prompt

**æ–‡ä»¶**ï¼š`prompts/extract_measurement_methods.txt`

```
ä½ æ˜¯ä¸€ä¸ªä¸“åˆ©åˆ†ææ–¹æ³•æå–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹è®ºæ–‡ä¸­æå–å˜é‡æµ‹é‡æ–¹æ³•ã€‚

è®ºæ–‡æ ‡é¢˜ï¼š{paper_title}
è®ºæ–‡å†…å®¹ï¼š{paper_content}

ç›®æ ‡å˜é‡åˆ—è¡¨ï¼š
{variable_list}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š

1. å˜é‡æµ‹é‡æ–¹æ³•
   - å˜é‡åç§°
   - æµ‹é‡æŒ‡æ ‡
   - è®¡ç®—å…¬å¼
   - æ•°æ®æ¥æº
   - è®¡ç®—æ­¥éª¤

2. ç»Ÿè®¡åˆ†ææ–¹æ³•
   - æ–¹æ³•åç§°
   - é€‚ç”¨åœºæ™¯
   - å…¬å¼
   - å®ç°æ­¥éª¤

è¾“å‡ºæ ¼å¼ï¼šJSON
{output_schema}
```

### Step 2ï¼šæ‰¹é‡æå–

**è„šæœ¬**ï¼š`scripts/extract_methods_batch.py`

**ä¼ªä»£ç **ï¼š
```python
import json
from pathlib import Path
import anthropic

# 1. åŠ è½½å˜é‡åˆ—è¡¨
with open('sandbox/static/data/causal_ontology_extracted.json') as f:
    ontology = json.load(f)
    variables = ontology['variables']

# 2. åŠ è½½å·²åˆ†æçš„è®ºæ–‡
paper_results = list(Path('batch_50_results').glob('*_analysis_result.json'))

# 3. åˆå§‹åŒ–Claude
client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# 4. æ‰¹é‡æå–
all_measurements = []
all_analysis_methods = []

for paper_file in paper_results:
    with open(paper_file) as f:
        paper_data = json.load(f)
    
    # æ„å»ºprompt
    prompt = build_extraction_prompt(paper_data, variables)
    
    # è°ƒç”¨Claude
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=4000,
        messages=[{"role": "user", "content": prompt}]
    )
    
    # è§£æç»“æœ
    extracted = json.loads(response.content[0].text)
    all_measurements.extend(extracted['measurements'])
    all_analysis_methods.extend(extracted['analysis_methods'])
    
    print(f"âœ“ å¤„ç†å®Œæˆ: {paper_file.name}")

# 5. åˆå¹¶å’Œå»é‡ï¼ˆå¤„ç†å¤šç§æ–¹æ³•ï¼‰
merged_measurements = merge_measurements_with_ranking(all_measurements)
merged_methods = merge_methods(all_analysis_methods)

def merge_measurements_with_ranking(all_measurements):
    """
    åˆå¹¶åŒä¸€å˜é‡çš„å¤šç§æµ‹é‡æ–¹æ³•ï¼Œå¹¶è®¡ç®—ä½¿ç”¨é¢‘ç‡å’Œæ¨èä¼˜å…ˆçº§
    """
    # æŒ‰å˜é‡åˆ†ç»„
    by_variable = {}
    for m in all_measurements:
        var_id = m['variable_id']
        if var_id not in by_variable:
            by_variable[var_id] = []
        by_variable[var_id].append(m)
    
    # å¤„ç†æ¯ä¸ªå˜é‡
    result = []
    for var_id, methods in by_variable.items():
        # æŒ‰æ–¹æ³•åç§°åˆ†ç»„ï¼ˆåŒä¸€æ–¹æ³•å¯èƒ½è¢«å¤šç¯‡è®ºæ–‡ä½¿ç”¨ï¼‰
        by_method_name = {}
        for m in methods:
            method_name = m['method_name']
            if method_name not in by_method_name:
                by_method_name[method_name] = {
                    'method': m,
                    'paper_count': 0,
                    'papers': []
                }
            by_method_name[method_name]['paper_count'] += 1
            by_method_name[method_name]['papers'].append(m.get('paper_title'))
        
        # è®¡ç®—ä½¿ç”¨é¢‘ç‡
        total_papers = len(paper_results)
        method_list = []
        for method_name, info in by_method_name.items():
            method = info['method']
            method['usage_frequency'] = info['paper_count'] / total_papers
            method['evidence']['paper_count'] = info['paper_count']
            method['evidence']['key_papers'] = info['papers'][:5]  # ä¿ç•™å‰5ç¯‡
            method_list.append(method)
        
        # æŒ‰ä½¿ç”¨é¢‘ç‡æ’åº
        method_list.sort(key=lambda m: m['usage_frequency'], reverse=True)
        
        # æ ‡æ³¨æ¨èçº§åˆ«
        if len(method_list) > 0:
            method_list[0]['recommendation_level'] = 'æ¨è'
            method_list[0]['recommendation_reason'] = f"æœ€å¸¸ç”¨æ–¹æ³•ï¼ˆ{method_list[0]['usage_frequency']:.0%}çš„è®ºæ–‡ä½¿ç”¨ï¼‰"
        if len(method_list) > 1:
            method_list[1]['recommendation_level'] = 'å¤‡é€‰'
            method_list[1]['recommendation_reason'] = f"æ¬¡å¸¸ç”¨æ–¹æ³•ï¼ˆ{method_list[1]['usage_frequency']:.0%}çš„è®ºæ–‡ä½¿ç”¨ï¼‰"
        if len(method_list) > 2:
            for m in method_list[2:]:
                m['recommendation_level'] = 'å¯é€‰'
                m['recommendation_reason'] = f"è¾ƒå°‘ä½¿ç”¨ï¼ˆ{m['usage_frequency']:.0%}çš„è®ºæ–‡ä½¿ç”¨ï¼‰"
        
        # æ·»åŠ æ–¹æ³•å¯¹æ¯”ä¿¡æ¯
        variable_entry = {
            'variable_id': var_id,
            'variable_name': methods[0]['variable_name'],
            'measurement_methods': method_list,
            'default_method': method_list[0]['method_id'] if method_list else None,
            'method_selection_logic': generate_selection_logic(method_list)
        }
        
        result.append(variable_entry)
    
    return result

# 6. ä¿å­˜ç»“æœ
save_json(merged_measurements, 'outputs/variable_measurement_methods.json')
save_json(merged_methods, 'outputs/statistical_analysis_methods.json')
```

### Step 3ï¼šéªŒè¯å’Œæ¸…æ´—

**è„šæœ¬**ï¼š`scripts/validate_extracted_methods.py`

**æ£€æŸ¥é¡¹**ï¼š
1. æ¯ä¸ªå˜é‡è‡³å°‘æœ‰1ä¸ªæµ‹é‡æ–¹æ³•
2. å…¬å¼æ ¼å¼æ­£ç¡®
3. Pythonä»£ç å¯æ‰§è¡Œ
4. æ•°æ®å­—æ®µåç§°åˆç†
5. **ä½¿ç”¨é¢‘ç‡æ€»å’Œåˆç†**ï¼ˆåŒä¸€å˜é‡çš„æ‰€æœ‰æ–¹æ³•é¢‘ç‡å’Œåº”â‰ˆ1.0ï¼‰
6. **æ¨èçº§åˆ«å·²æ ‡æ³¨**ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ª"æ¨è"æ–¹æ³•ï¼‰
5. æ–‡çŒ®å¼•ç”¨å®Œæ•´

### Step 4ï¼šå¯¼å…¥Neo4j

**è„šæœ¬**ï¼š`scripts/import_method_graph_v2.py`

**Cypherè¯­å¥ç¤ºä¾‹**ï¼š
```cypher
// åˆ›å»ºVariableèŠ‚ç‚¹
CREATE (v:Variable {
  id: 'V09_tech_diversity',
  name: 'æŠ€æœ¯è·¨ç•Œåº¦',
  category: 'mediator'
})

// åˆ›å»ºMeasurementMethodèŠ‚ç‚¹
CREATE (m:MeasurementMethod {
  id: 'M001',
  name: 'Shannon Entropy',
  formula: '-SUM(p_i * log(p_i))',
  code: '...'
})

// åˆ›å»ºå…³ç³»
CREATE (v)-[:HAS_MEASUREMENT]->(m)

// åˆ›å»ºDataFieldèŠ‚ç‚¹
CREATE (d:DataField {
  name: 'IPCä¸»åˆ†ç±»å·',
  type: 'string',
  format: 'åˆ†å·åˆ†éš”'
})

// åˆ›å»ºå…³ç³»
CREATE (m)-[:REQUIRES_DATA]->(d)
```

---

## â±ï¸ æ—¶é—´ä¼°ç®—

| æ­¥éª¤ | æ—¶é—´ | è¯´æ˜ |
|------|------|------|
| è®¾è®¡Prompt | 2å°æ—¶ | ç¼–å†™æå–æ¨¡æ¿å’Œç¤ºä¾‹ |
| ç¼–å†™æå–è„šæœ¬ | 2å°æ—¶ | åŸºäºå·²æœ‰è„šæœ¬ä¿®æ”¹ |
| è¿è¡Œæå–ï¼ˆ50ç¯‡ï¼‰ | 1å°æ—¶ | APIè°ƒç”¨ï¼Œè‡ªåŠ¨åŒ– |
| éªŒè¯å’Œæ¸…æ´— | 3å°æ—¶ | äººå·¥æ£€æŸ¥å’Œä¿®æ­£ |
| å¯¼å…¥Neo4j | 1å°æ—¶ | ç¼–å†™å¯¼å…¥è„šæœ¬ |
| æµ‹è¯•éªŒè¯ | 2å°æ—¶ | ç«¯åˆ°ç«¯æµ‹è¯• |
| **æ€»è®¡** | **11å°æ—¶** | **çº¦1.5å¤©** |

---

## ğŸ’° æˆæœ¬ä¼°ç®—

**Claude APIè°ƒç”¨ï¼š**
- 50ç¯‡è®ºæ–‡ Ã— 4000 tokens/ç¯‡ = 200K tokensè¾“å…¥
- 50ç¯‡è®ºæ–‡ Ã— 2000 tokens/ç¯‡ = 100K tokensè¾“å‡º
- æˆæœ¬ï¼šçº¦ $3-5

---

## âœ… éªŒæ”¶æ ‡å‡†

1. **è¦†ç›–ç‡**ï¼š30ä¸ªå˜é‡ä¸­è‡³å°‘25ä¸ªæœ‰æµ‹é‡æ–¹æ³•
2. **è´¨é‡**ï¼šæ¯ä¸ªæ–¹æ³•éƒ½æœ‰å…¬å¼ã€ä»£ç ã€æ–‡çŒ®æ”¯æŒ
3. **å¯ç”¨æ€§**ï¼šPythonä»£ç å¯ä»¥ç›´æ¥è¿è¡Œ
4. **å®Œæ•´æ€§**ï¼šåŒ…å«æ•°æ®å­—æ®µæ˜ å°„å’ŒExcelåˆ—å
5. **å¯è¿½æº¯**ï¼šæ¯ä¸ªæ–¹æ³•éƒ½èƒ½è¿½æº¯åˆ°åŸæ–‡çŒ®

---

## ğŸš€ ä¸‹ä¸€æ­¥

æå–å®Œæˆåï¼Œå¯ä»¥ï¼š
1. æ›´æ–°Strategistï¼Œä½¿ç”¨æ–°çš„æ–¹æ³•å›¾è°±
2. æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹
3. ç”Ÿæˆç¤ºä¾‹ç ”ç©¶æŠ¥å‘Š

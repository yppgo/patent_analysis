#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å› æœå…³ç³»æŠ½å–å™¨ - ä»ä¸“åˆ©åˆ†æè®ºæ–‡ä¸­æå–å› æœå‡è®¾
ç”¨äºæ„å»ºå’Œå®Œå–„å› æœçŸ¥è¯†å›¾è°±

åŠŸèƒ½ï¼š
1. ä»50ç¯‡è®ºæ–‡çš„åˆ†æç»“æœä¸­æå–å› æœå…³ç³»
2. ç»Ÿè®¡å˜é‡å‡ºç°é¢‘æ¬¡
3. ç»Ÿè®¡å› æœè·¯å¾„çš„éªŒè¯æƒ…å†µ
4. ç”Ÿæˆå®Œå–„åçš„å› æœå›¾è°±
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple
from collections import defaultdict
from dotenv import load_dotenv

load_dotenv()


class CausalRelationExtractor:
    """ä»è®ºæ–‡åˆ†æç»“æœä¸­æŠ½å–å› æœå…³ç³»"""
    
    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–æŠ½å–å™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆç”¨äºæ·±åº¦åˆ†æï¼‰
        """
        self.llm = llm_client
        
        # å˜é‡æ˜ å°„è¡¨ï¼šå°†è®ºæ–‡ä¸­çš„å…·ä½“æŒ‡æ ‡æ˜ å°„åˆ°æŠ½è±¡å˜é‡
        self.variable_mapping = {
            # è¾“å…¥å˜é‡
            "ä¸“åˆ©æ•°é‡": "tech_intensity",
            "patent count": "tech_intensity",
            "ç”³è¯·äººè§„æ¨¡": "firm_size",
            "ä¼ä¸šè§„æ¨¡": "firm_size",
            "ç ”å‘æŠ•å…¥": "rd_investment",
            "R&D": "rd_investment",
            "å›½é™…åˆä½œ": "international_collab",
            "international collaboration": "international_collab",
            "äº§å­¦ç ”": "university_collab",
            "university": "university_collab",
            
            # ä¸­ä»‹å˜é‡
            "IPCç†µ": "tech_diversity",
            "æŠ€æœ¯å¤šæ ·æ€§": "tech_diversity",
            "æŠ€æœ¯è·¨ç•Œ": "tech_diversity",
            "diversity": "tech_diversity",
            "NPL": "science_linkage",
            "ç§‘å­¦å¼•ç”¨": "science_linkage",
            "science linkage": "science_linkage",
            "TCT": "tech_cycle_time",
            "æŠ€æœ¯å‘¨æœŸ": "tech_cycle_time",
            "cycle time": "tech_cycle_time",
            
            # ç»“æœå˜é‡
            "å¼•ç”¨": "tech_impact",
            "citation": "tech_impact",
            "è¢«å¼•": "tech_impact",
            "å½±å“åŠ›": "tech_impact",
            "impact": "tech_impact",
            "ä¸“åˆ©ä»·å€¼": "commercial_value",
            "patent value": "commercial_value",
            "ç»´æŒå¹´é™": "commercial_value",
        }
        
        # å˜é‡å®šä¹‰
        self.variable_definitions = {
            "tech_intensity": {"label": "æŠ€æœ¯æŠ•å…¥å¼ºåº¦", "category": "input"},
            "firm_size": {"label": "ä¼ä¸šè§„æ¨¡", "category": "input"},
            "rd_investment": {"label": "ç ”å‘æŠ•èµ„", "category": "input"},
            "international_collab": {"label": "å›½é™…åˆä½œ", "category": "input"},
            "university_collab": {"label": "äº§å­¦ç ”åˆä½œ", "category": "input"},
            "tech_diversity": {"label": "æŠ€æœ¯è·¨ç•Œåº¦", "category": "mediator"},
            "science_linkage": {"label": "ç§‘å­¦å…³è”åº¦", "category": "mediator"},
            "tech_cycle_time": {"label": "æŠ€æœ¯è¿­ä»£é€Ÿåº¦", "category": "mediator"},
            "tech_impact": {"label": "æŠ€æœ¯å½±å“åŠ›", "category": "outcome"},
            "commercial_value": {"label": "å•†ä¸šä»·å€¼", "category": "outcome"},
        }
        
        # ç»Ÿè®¡æ•°æ®
        self.variable_counts = defaultdict(int)
        self.path_counts = defaultdict(lambda: {"count": 0, "papers": [], "domains": set()})
        self.domain_counts = defaultdict(int)
    
    def extract_from_folder(self, folder_path: str) -> Dict[str, Any]:
        """
        ä»æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è®ºæ–‡åˆ†æç»“æœä¸­æŠ½å–å› æœå…³ç³»
        
        Args:
            folder_path: åŒ…å«åˆ†æç»“æœJSONçš„æ–‡ä»¶å¤¹
            
        Returns:
            æŠ½å–ç»“æœç»Ÿè®¡
        """
        folder = Path(folder_path)
        json_files = list(folder.glob("*_analysis_result.json"))
        
        print(f"æ‰¾åˆ° {len(json_files)} ä¸ªåˆ†æç»“æœæ–‡ä»¶")
        print("=" * 60)
        
        for idx, json_file in enumerate(json_files, 1):
            try:
                print(f"[{idx}/{len(json_files)}] å¤„ç†: {json_file.name[:50]}...")
                
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self._extract_from_paper(data)
                
            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
        
        return self._generate_statistics()
    
    def _extract_from_paper(self, paper_data: Dict) -> None:
        """ä»å•ç¯‡è®ºæ–‡ä¸­æŠ½å–ä¿¡æ¯"""
        
        paper_title = paper_data.get("paper_meta", {}).get("title", "Unknown")
        
        # æå–ç ”ç©¶é¢†åŸŸ
        domain = self._extract_domain(paper_data)
        if domain:
            self.domain_counts[domain] += 1
        
        # æå–åˆ†ææ­¥éª¤ä¸­çš„å˜é‡
        for step in paper_data.get("analysis_logic_chains", []):
            self._extract_variables_from_step(step, paper_title, domain)
    
    def _extract_domain(self, paper_data: Dict) -> str:
        """ä»è®ºæ–‡æ•°æ®ä¸­æå–ç ”ç©¶é¢†åŸŸ"""
        
        # å°è¯•ä»æ•°æ®é›†é…ç½®ä¸­æå–
        dataset_config = paper_data.get("dataset_config", {})
        dataset_name = dataset_config.get("name", "")
        query_condition = dataset_config.get("query_condition", "")
        
        # ç®€å•çš„é¢†åŸŸè¯†åˆ«è§„åˆ™
        text = f"{dataset_name} {query_condition}".lower()
        
        if any(kw in text for kw in ["clean", "green", "energy", "solar", "wind"]):
            return "Clean Energy"
        elif any(kw in text for kw in ["bio", "pharma", "medical", "health"]):
            return "Biotech"
        elif any(kw in text for kw in ["ict", "telecom", "5g", "iot", "ai", "machine learning"]):
            return "ICT"
        elif any(kw in text for kw in ["material", "nano"]):
            return "Materials"
        elif any(kw in text for kw in ["automotive", "vehicle", "car"]):
            return "Automotive"
        else:
            return "General"
    
    def _extract_variables_from_step(self, step: Dict, paper_title: str, domain: str) -> None:
        """ä»åˆ†ææ­¥éª¤ä¸­æå–å˜é‡"""
        
        method_name = step.get("method_name", "")
        objective = step.get("objective", "")
        inputs = step.get("inputs", [])
        metrics = step.get("evaluation_metrics", [])
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ç”¨äºå˜é‡è¯†åˆ«
        all_text = f"{method_name} {objective} {' '.join(inputs)}"
        
        # è¯†åˆ«å˜é‡
        found_variables = []
        for keyword, var_id in self.variable_mapping.items():
            if keyword.lower() in all_text.lower():
                found_variables.append(var_id)
                self.variable_counts[var_id] += 1
        
        # å¦‚æœæ‰¾åˆ°å¤šä¸ªå˜é‡ï¼Œå°è¯•æ¨æ–­å› æœå…³ç³»
        if len(found_variables) >= 2:
            # ç®€å•è§„åˆ™ï¼šå‡è®¾ç¬¬ä¸€ä¸ªæ˜¯è‡ªå˜é‡ï¼Œæœ€åä¸€ä¸ªæ˜¯å› å˜é‡
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¯å‘å¼è§„åˆ™
            for i, var1 in enumerate(found_variables[:-1]):
                for var2 in found_variables[i+1:]:
                    # æ£€æŸ¥å˜é‡ç±»åˆ«ï¼Œç¡®ä¿æ–¹å‘åˆç†
                    cat1 = self.variable_definitions.get(var1, {}).get("category", "")
                    cat2 = self.variable_definitions.get(var2, {}).get("category", "")
                    
                    # åªè®°å½•åˆç†çš„å› æœæ–¹å‘
                    if self._is_valid_causal_direction(cat1, cat2):
                        path_key = f"{var1} -> {var2}"
                        self.path_counts[path_key]["count"] += 1
                        self.path_counts[path_key]["papers"].append(paper_title)
                        if domain:
                            self.path_counts[path_key]["domains"].add(domain)
    
    def _is_valid_causal_direction(self, cat1: str, cat2: str) -> bool:
        """æ£€æŸ¥å› æœæ–¹å‘æ˜¯å¦åˆç†"""
        
        # å®šä¹‰åˆç†çš„å› æœæ–¹å‘
        valid_directions = {
            ("input", "mediator"),
            ("input", "outcome"),
            ("mediator", "outcome"),
            ("mediator", "mediator"),
        }
        
        return (cat1, cat2) in valid_directions
    
    def _generate_statistics(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»Ÿè®¡ç»“æœ"""
        
        return {
            "variable_counts": dict(self.variable_counts),
            "path_counts": {
                k: {
                    "count": v["count"],
                    "papers": v["papers"][:5],  # åªä¿ç•™å‰5ç¯‡
                    "domains": list(v["domains"])
                }
                for k, v in sorted(
                    self.path_counts.items(),
                    key=lambda x: x[1]["count"],
                    reverse=True
                )
            },
            "domain_counts": dict(self.domain_counts),
            "total_papers": sum(self.domain_counts.values())
        }
    
    def generate_ontology(self, output_path: str = None) -> Dict[str, Any]:
        """
        åŸºäºæŠ½å–ç»“æœç”Ÿæˆå› æœæœ¬ä½“è®º
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ç”Ÿæˆçš„æœ¬ä½“è®º
        """
        
        # æ„å»ºå˜é‡åˆ—è¡¨
        variables = []
        for var_id, count in sorted(self.variable_counts.items(), key=lambda x: x[1], reverse=True):
            var_def = self.variable_definitions.get(var_id, {})
            variables.append({
                "id": var_id,
                "label": var_def.get("label", var_id),
                "category": var_def.get("category", "unknown"),
                "evidence_count": count,
                "source": "extracted_from_papers"
            })
        
        # æ„å»ºå› æœè·¯å¾„åˆ—è¡¨
        causal_paths = []
        for path_key, path_data in self.path_counts.items():
            source, target = path_key.split(" -> ")
            causal_paths.append({
                "path_id": f"P_{len(causal_paths)+1:02d}",
                "source": source,
                "target": target,
                "evidence": {
                    "validated": path_data["count"] >= 3,  # 3ç¯‡ä»¥ä¸Šè§†ä¸ºå·²éªŒè¯
                    "evidence_count": path_data["count"],
                    "sample_papers": path_data["papers"][:3],
                    "validated_domains": list(path_data["domains"])  # setè½¬list
                },
                "source": "extracted_from_papers"
            })
        
        ontology = {
            "meta": {
                "name": "Extracted Causal Ontology",
                "version": "1.0",
                "description": "ä»50ç¯‡ä¸“åˆ©åˆ†æè®ºæ–‡ä¸­è‡ªåŠ¨æŠ½å–çš„å› æœå…³ç³»",
                "extraction_method": "rule_based + keyword_matching",
                "total_papers_analyzed": sum(self.domain_counts.values())
            },
            "variables": variables,
            "causal_paths": causal_paths,
            "statistics": {
                "total_variables": len(variables),
                "total_paths": len(causal_paths),
                "validated_paths": sum(1 for p in causal_paths if p["evidence"]["validated"]),
                "domain_coverage": dict(self.domain_counts)
            }
        }
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(ontology, f, ensure_ascii=False, indent=2)
            print(f"\nâœ“ æœ¬ä½“è®ºå·²ä¿å­˜åˆ°: {output_path}")
        
        return ontology


def extract_with_llm(paper_data: Dict, llm_client) -> Dict[str, Any]:
    """
    ä½¿ç”¨LLMæ·±åº¦æŠ½å–å› æœå…³ç³»ï¼ˆæ›´å‡†ç¡®ä½†æ›´æ…¢ï¼‰
    
    Args:
        paper_data: è®ºæ–‡åˆ†æç»“æœ
        llm_client: LLMå®¢æˆ·ç«¯
        
    Returns:
        æŠ½å–çš„å› æœå…³ç³»
    """
    
    prompt = f"""
# ä»»åŠ¡
ä»ä»¥ä¸‹ä¸“åˆ©åˆ†æè®ºæ–‡çš„åˆ†æç»“æœä¸­ï¼Œæå–ç ”ç©¶å‡è®¾å’Œå› æœå…³ç³»ã€‚

# è®ºæ–‡ä¿¡æ¯
æ ‡é¢˜: {paper_data.get('paper_meta', {}).get('title', 'Unknown')}

# åˆ†ææ­¥éª¤
{json.dumps(paper_data.get('analysis_logic_chains', []), ensure_ascii=False, indent=2)}

# è¦æ±‚
1. è¯†åˆ«è®ºæ–‡ä¸­éšå«çš„ç ”ç©¶å‡è®¾ï¼ˆå¦‚æœæœ‰ï¼‰
2. è¯†åˆ«è‡ªå˜é‡å’Œå› å˜é‡
3. åˆ¤æ–­å› æœæ•ˆåº”çš„æ–¹å‘ï¼ˆæ­£å‘/è´Ÿå‘ï¼‰
4. è¯†åˆ«ç ”ç©¶çš„æŠ€æœ¯é¢†åŸŸ

# è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
{{
  "domain": "æŠ€æœ¯é¢†åŸŸï¼ˆå¦‚ ICT, Biotech, Clean Energyï¼‰",
  "hypotheses": [
    {{
      "independent_var": "è‡ªå˜é‡åç§°",
      "dependent_var": "å› å˜é‡åç§°",
      "effect_direction": "positive/negative/unknown",
      "confidence": 0.8,
      "evidence": "ä»è®ºæ–‡ä¸­æå–çš„è¯æ®"
    }}
  ]
}}

åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
    
    try:
        response = llm_client.invoke(prompt)
        content = response.content if hasattr(response, 'content') else str(response)
        
        # æ¸…ç†å“åº”
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
            content = content.strip()
        
        return json.loads(content)
    except Exception as e:
        print(f"LLMæŠ½å–å¤±è´¥: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    
    print("=" * 60)
    print("å› æœå…³ç³»æŠ½å–å™¨ - ä»ä¸“åˆ©åˆ†æè®ºæ–‡ä¸­æå–å› æœå‡è®¾")
    print("=" * 60)
    
    # åˆ›å»ºæŠ½å–å™¨
    extractor = CausalRelationExtractor()
    
    # ä»batch_50_resultsæ–‡ä»¶å¤¹æŠ½å–
    folder_path = "batch_50_results"
    
    if not Path(folder_path).exists():
        print(f"é”™è¯¯: æ–‡ä»¶å¤¹ä¸å­˜åœ¨ - {folder_path}")
        return
    
    # æ‰§è¡ŒæŠ½å–
    stats = extractor.extract_from_folder(folder_path)
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("æŠ½å–ç»“æœç»Ÿè®¡")
    print("=" * 60)
    
    print(f"\nğŸ“Š åˆ†æè®ºæ–‡æ•°: {stats['total_papers']}")
    
    print(f"\nğŸ“‹ å˜é‡å‡ºç°é¢‘æ¬¡:")
    for var_id, count in sorted(stats['variable_counts'].items(), key=lambda x: x[1], reverse=True):
        var_def = extractor.variable_definitions.get(var_id, {})
        print(f"  - {var_def.get('label', var_id)}: {count}æ¬¡")
    
    print(f"\nğŸ”— å› æœè·¯å¾„ç»Ÿè®¡ (Top 10):")
    for i, (path, data) in enumerate(list(stats['path_counts'].items())[:10], 1):
        validated = "âœ“" if data['count'] >= 3 else "?"
        print(f"  {i}. [{validated}] {path}: {data['count']}ç¯‡è®ºæ–‡")
        print(f"      é¢†åŸŸ: {', '.join(data['domains']) if data['domains'] else 'æœªçŸ¥'}")
    
    print(f"\nğŸŒ é¢†åŸŸåˆ†å¸ƒ:")
    for domain, count in sorted(stats['domain_counts'].items(), key=lambda x: x[1], reverse=True):
        print(f"  - {domain}: {count}ç¯‡")
    
    # ç”Ÿæˆæœ¬ä½“è®º
    output_path = "sandbox/static/data/extracted_causal_ontology.json"
    ontology = extractor.generate_ontology(output_path)
    
    print(f"\nâœ… æŠ½å–å®Œæˆ!")
    print(f"  - å˜é‡æ•°: {ontology['statistics']['total_variables']}")
    print(f"  - è·¯å¾„æ•°: {ontology['statistics']['total_paths']}")
    print(f"  - å·²éªŒè¯è·¯å¾„: {ontology['statistics']['validated_paths']}")


if __name__ == "__main__":
    main()

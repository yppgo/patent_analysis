"""
ç”Ÿäº§çº§ Neo4j çŸ¥è¯†å›¾è°±å…¥åº“è„šæœ¬
Patent-DeepScientist é¡¹ç›®

æ¶æ„è®¾è®¡:
- åŸºç¡€è®¾æ–½èŠ‚ç‚¹ (å…¨å±€å…±äº«): Intent, Method, Dataset, Data - ä½¿ç”¨ MERGE
- å®ä¾‹èŠ‚ç‚¹ (åŠ¨æ€åˆ›å»º): Paper, AnalysisEvent, Conclusion - ä½¿ç”¨ CREATE
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
from neo4j import GraphDatabase
from neo4j.exceptions import ServiceUnavailable, AuthError


class KnowledgeGraphIngester:
    """çŸ¥è¯†å›¾è°±å…¥åº“å™¨ - ç”Ÿäº§çº§å®ç°"""
    
    def __init__(self, uri: str, user: str, password: str):
        """
        åˆå§‹åŒ– Neo4j è¿æ¥
        
        Args:
            uri: Neo4j æ•°æ®åº“åœ°å€ (ä¾‹å¦‚: bolt://localhost:7687)
            user: ç”¨æˆ·å
            password: å¯†ç 
        """
        try:
            self.driver = GraphDatabase.driver(uri, auth=(user, password))
            # æµ‹è¯•è¿æ¥
            self.driver.verify_connectivity()
            print("âœ“ Neo4j è¿æ¥æˆåŠŸ")
        except AuthError:
            raise Exception("âŒ Neo4j è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç ")
        except ServiceUnavailable:
            raise Exception("âŒ Neo4j æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å¯åŠ¨")
        except Exception as e:
            raise Exception(f"âŒ Neo4j è¿æ¥å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.driver:
            self.driver.close()
            print("âœ“ Neo4j è¿æ¥å·²å…³é—­")
    
    def clear_database(self):
        """
        æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®
        âš ï¸ è­¦å‘Šï¼šæ­¤æ“ä½œä¸å¯é€†ï¼
        """
        print("\nâš ï¸  è­¦å‘Šï¼šå³å°†æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®ï¼")
        confirm = input("ç¡®è®¤æ¸…ç©ºï¼Ÿè¾“å…¥ 'YES' ç»§ç»­: ")
        
        if confirm == "YES":
            with self.driver.session() as session:
                session.run("MATCH (n) DETACH DELETE n")
            print("âœ“ æ•°æ®åº“å·²æ¸…ç©º")
        else:
            print("âœ— æ“ä½œå·²å–æ¶ˆ")
    
    def initialize_schema(self):
        """
        åˆå§‹åŒ–æ•°æ®åº“ Schema
        åˆ›å»ºå”¯ä¸€æ€§çº¦æŸï¼Œç¡®ä¿å…¨å±€èŠ‚ç‚¹çš„å”¯ä¸€æ€§
        """
        constraints = [
            "CREATE CONSTRAINT intent_name IF NOT EXISTS FOR (i:Intent) REQUIRE i.name IS UNIQUE",
            "CREATE CONSTRAINT method_name IF NOT EXISTS FOR (m:Method) REQUIRE m.name IS UNIQUE",
            "CREATE CONSTRAINT dataset_name IF NOT EXISTS FOR (d:Dataset) REQUIRE d.name IS UNIQUE",
            "CREATE CONSTRAINT data_name IF NOT EXISTS FOR (dt:Data) REQUIRE dt.name IS UNIQUE",
            "CREATE CONSTRAINT paper_title IF NOT EXISTS FOR (p:Paper) REQUIRE p.title IS UNIQUE"
        ]
        
        with self.driver.session() as session:
            for constraint in constraints:
                try:
                    session.run(constraint)
                    print(f"  âœ“ çº¦æŸåˆ›å»ºæˆåŠŸ: {constraint.split('(')[1].split(')')[0]}")
                except Exception as e:
                    # çº¦æŸå¯èƒ½å·²å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
                    if "already exists" not in str(e).lower():
                        print(f"  âš  çº¦æŸåˆ›å»ºè­¦å‘Š: {e}")
        
        print("âœ“ Schema åˆå§‹åŒ–å®Œæˆ")
    
    def ingest_paper(self, json_data: Dict[str, Any]) -> bool:
        """
        å…¥åº“å•ç¯‡è®ºæ–‡çš„åˆ†ææ•°æ®
        
        Args:
            json_data: åŒ…å« paper_meta, dataset_config å’Œ analysis_logic_chains çš„å­—å…¸
        
        Returns:
            bool: å…¥åº“æ˜¯å¦æˆåŠŸ
        """
        try:
            paper_meta = json_data.get("paper_meta", {})
            dataset_config = json_data.get("dataset_config", {})
            logic_chains = json_data.get("analysis_logic_chains", [])
            
            paper_title = paper_meta.get("title", "")
            paper_year = paper_meta.get("year", "")
            
            if not paper_title:
                print("  âš  è­¦å‘Š: è®ºæ–‡æ ‡é¢˜ä¸ºç©ºï¼Œè·³è¿‡")
                return False
            
            with self.driver.session() as session:
                # 1. åˆ›å»º/é”å®š Paper èŠ‚ç‚¹
                session.execute_write(
                    self._create_paper_node,
                    paper_title,
                    paper_year
                )
                
                # 2. å¤„ç† Dataset å…³ç³»
                if dataset_config:
                    dataset_source = dataset_config.get("source", "")
                    if dataset_source:
                        session.execute_write(
                            self._link_paper_to_dataset,
                            paper_title,
                            dataset_source,
                            dataset_config
                        )
                
                # 3. å¤„ç†æ¯ä¸ªåˆ†ææ­¥éª¤
                for step in logic_chains:
                    session.execute_write(
                        self._ingest_analysis_step,
                        paper_title,
                        step
                    )
            
            print(f"  âœ“ æˆåŠŸå…¥åº“: {paper_title[:60]}...")
            return True
            
        except Exception as e:
            print(f"  âœ— å…¥åº“å¤±è´¥: {e}")
            return False
    
    @staticmethod
    def _create_paper_node(tx, title: str, year: str):
        """
        åˆ›å»º Paper èŠ‚ç‚¹ (ä½¿ç”¨ MERGE é¿å…é‡å¤)
        
        Args:
            tx: Neo4j äº‹åŠ¡
            title: è®ºæ–‡æ ‡é¢˜
            year: å‘è¡¨å¹´ä»½
        """
        query = """
        MERGE (p:Paper {title: $title})
        ON CREATE SET 
            p.year = $year,
            p.created_at = datetime()
        ON MATCH SET
            p.updated_at = datetime()
        RETURN p
        """
        tx.run(query, title=title, year=year)
    
    @staticmethod
    def _link_paper_to_dataset(tx, paper_title: str, dataset_source: str, dataset_config: Dict):
        """
        è¿æ¥ Paper åˆ°å…¨å±€ Dataset èŠ‚ç‚¹
        
        Args:
            tx: Neo4j äº‹åŠ¡
            paper_title: è®ºæ–‡æ ‡é¢˜
            dataset_source: æ•°æ®é›†æ¥æº (ä¾‹å¦‚: USPTO, EPO)
            dataset_config: æ•°æ®é›†é…ç½®ä¿¡æ¯
        """
        query = """
        MATCH (p:Paper {title: $paper_title})
        MERGE (d:Dataset {name: $dataset_source})
        ON CREATE SET
            d.created_at = datetime()
        MERGE (p)-[r:USES_DATASET]->(d)
        ON CREATE SET
            r.dataset_id = $dataset_id,
            r.query_condition = $query_condition,
            r.size = $size,
            r.time_range = $time_range,
            r.preprocessing = $preprocessing,
            r.notes = $notes,
            r.created_at = datetime()
        """
        tx.run(
            query,
            paper_title=paper_title,
            dataset_source=dataset_source,
            dataset_id=dataset_config.get("dataset_id", ""),
            query_condition=dataset_config.get("query_condition", ""),
            size=dataset_config.get("size", ""),
            time_range=dataset_config.get("time_range", ""),
            preprocessing=dataset_config.get("preprocessing", ""),
            notes=dataset_config.get("notes", "")
        )
    
    @staticmethod
    def _ingest_analysis_step(tx, paper_title: str, step: Dict):
        """
        å…¥åº“å•ä¸ªåˆ†ææ­¥éª¤ (æ ¸å¿ƒé€»è¾‘)
        
        æ¶æ„:
        1. é”å®š Paper èŠ‚ç‚¹
        2. é”å®šå…¨å±€åŸºç¡€è®¾æ–½èŠ‚ç‚¹ (Intent, Method, Dataset)
        3. åˆ›å»ºåŠ¨æ€å®ä¾‹èŠ‚ç‚¹ (AnalysisEvent, Conclusion)
        4. å»ºç«‹å…³ç³»é“¾
        5. å¤„ç† Data èŠ‚ç‚¹åˆ—è¡¨ (ä½¿ç”¨ UNWIND)
        
        Args:
            tx: Neo4j äº‹åŠ¡
            paper_title: è®ºæ–‡æ ‡é¢˜
            step: åˆ†ææ­¥éª¤å­—å…¸
        """
        # æå–æ­¥éª¤ä¿¡æ¯
        step_id = step.get("step_id", 0)
        objective = step.get("objective", "")
        standardized_intent = step.get("standardized_intent", "")
        method_name = step.get("method_name", "")
        derived_conclusion = step.get("derived_conclusion", "")
        
        # æå–é…ç½®å’ŒæŒ‡æ ‡ (è½¬ä¸º JSON å­—ç¬¦ä¸²)
        implementation_config = step.get("implementation_config", {})
        config_json = json.dumps(implementation_config, ensure_ascii=False)
        
        evaluation_metrics = step.get("evaluation_metrics", [])
        metrics_json = json.dumps(evaluation_metrics, ensure_ascii=False)
        
        # æå–è¾“å…¥æ•°æ®åˆ—è¡¨
        inputs = step.get("inputs", [])
        if not isinstance(inputs, list):
            inputs = []
        
        # ä¸» Cypher æŸ¥è¯¢ - åŸå­æ“ä½œ
        query = """
        // 1. é”å®š Paper èŠ‚ç‚¹
        MATCH (p:Paper {title: $paper_title})
        
        // 2. é”å®šå…¨å±€åŸºç¡€è®¾æ–½èŠ‚ç‚¹ (The Fixed Infrastructure)
        MERGE (i:Intent {name: $intent_name})
        ON CREATE SET i.created_at = datetime()
        
        MERGE (m:Method {name: $method_name})
        ON CREATE SET m.created_at = datetime()
        
        // 3. åˆ›å»ºåŠ¨æ€å®ä¾‹èŠ‚ç‚¹ (AnalysisEvent)
        CREATE (e:AnalysisEvent {
            step_id: $step_id,
            objective: $objective,
            config: $config,
            metrics: $metrics,
            success_confidence: $success_confidence,
            created_at: datetime()
        })
        
        // 4. åˆ›å»º Conclusion èŠ‚ç‚¹
        CREATE (c:Conclusion {
            text: $conclusion_text,
            created_at: datetime()
        })
        
        // 5. å»ºç«‹å…³ç³»é“¾
        MERGE (p)-[:CONTAINS_EVENT]->(e)
        MERGE (e)-[:TARGETS_INTENT]->(i)
        MERGE (e)-[:USES_METHOD]->(m)
        MERGE (e)-[:PRODUCED_CONCLUSION]->(c)
        MERGE (c)-[:ADDRESSES_INTENT]->(i)
        
        // 6. å¤„ç† Data èŠ‚ç‚¹åˆ—è¡¨ (ä½¿ç”¨ FOREACH + MERGE)
        FOREACH (input_name IN $input_list |
            MERGE (dt:Data {name: input_name})
            ON CREATE SET dt.created_at = datetime()
            MERGE (e)-[:REQUIRES_INPUT]->(dt)
        )
        
        RETURN e, c
        """
        
        # æ‰§è¡ŒæŸ¥è¯¢
        tx.run(
            query,
            paper_title=paper_title,
            step_id=step_id,
            objective=objective,
            intent_name=standardized_intent if standardized_intent else "æœªåˆ†ç±»æ„å›¾",
            method_name=method_name if method_name else "æœªæŒ‡å®šæ–¹æ³•",
            config=config_json,
            metrics=metrics_json,
            success_confidence=step.get("success_confidence", 0.0),
            conclusion_text=derived_conclusion if derived_conclusion else "æ— ç»“è®º",
            input_list=inputs  # ä¼ é€’åˆ—è¡¨ç»™ FOREACH
        )
    
    def batch_ingest_from_folder(self, folder_path: str, pattern: str = "*_analysis_result.json") -> Dict[str, int]:
        """
        æ‰¹é‡å…¥åº“æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ JSON æ–‡ä»¶
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *_analysis_result.json)
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯ {"success": æˆåŠŸæ•°, "failed": å¤±è´¥æ•°, "total": æ€»æ•°}
        """
        folder = Path(folder_path)
        
        if not folder.exists():
            raise FileNotFoundError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        
        # è·å–æ‰€æœ‰åŒ¹é…çš„ JSON æ–‡ä»¶
        json_files = list(folder.glob(pattern))
        
        if not json_files:
            print(f"âš  è­¦å‘Š: åœ¨ {folder_path} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é… '{pattern}' çš„æ–‡ä»¶")
            return {"success": 0, "failed": 0, "total": 0}
        
        print(f"\n{'='*70}")
        print(f"å¼€å§‹æ‰¹é‡å…¥åº“: æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
        print(f"{'='*70}\n")
        
        success_count = 0
        failed_count = 0
        
        for idx, json_file in enumerate(json_files, 1):
            print(f"[{idx}/{len(json_files)}] å¤„ç†: {json_file.name[:60]}...")
            
            try:
                # è¯»å– JSON æ–‡ä»¶
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # å…¥åº“
                if self.ingest_paper(data):
                    success_count += 1
                else:
                    failed_count += 1
                    
            except json.JSONDecodeError as e:
                print(f"  âœ— JSON è§£æé”™è¯¯: {e}")
                failed_count += 1
            except Exception as e:
                print(f"  âœ— å¤„ç†å¤±è´¥: {type(e).__name__}: {e}")
                failed_count += 1
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*70}")
        print(f"æ‰¹é‡å…¥åº“å®Œæˆ!")
        print(f"  âœ“ æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
        print(f"  âœ— å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        print(f"  æ€»è®¡: {len(json_files)} ä¸ªæ–‡ä»¶")
        print(f"{'='*70}\n")
        
        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(json_files)
        }


def main():
    """ä¸»å‡½æ•° - æ‰¹é‡å…¥åº“ 50 æ¡æ•°æ®"""
    
    import sys
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½
    try:
        from neo4j_config import NEO4J_CONFIG
        uri = NEO4J_CONFIG["uri"]
        user = NEO4J_CONFIG["user"]
        password = NEO4J_CONFIG["password"]
    except ImportError:
        print("âš  è­¦å‘Š: æœªæ‰¾åˆ° neo4j_config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        uri = "bolt://localhost:7687"
        user = "neo4j"
        password = "12345678"
    
    # æ‰¹é‡å…¥åº“æ–‡ä»¶å¤¹
    batch_folder = "batch_50_results"
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Patent-DeepScientist çŸ¥è¯†å›¾è°±å…¥åº“å·¥å…· v1.0                      â•‘
â•‘  ç”Ÿäº§çº§ Neo4j æ‰¹é‡å…¥åº“è„šæœ¬                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åˆ›å»ºå…¥åº“å™¨
    ingester = None
    try:
        ingester = KnowledgeGraphIngester(uri, user, password)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç©ºæ•°æ®åº“
        if "--clear" in sys.argv:
            ingester.clear_database()
        
        # åˆå§‹åŒ– Schema
        print("\n[æ­¥éª¤ 1/2] åˆå§‹åŒ–æ•°æ®åº“ Schema...")
        ingester.initialize_schema()
        
        # æ‰¹é‡å…¥åº“
        print(f"\n[æ­¥éª¤ 2/2] æ‰¹é‡å…¥åº“æ•°æ®...")
        stats = ingester.batch_ingest_from_folder(batch_folder)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        if stats["success"] > 0:
            print(f"\nğŸ‰ å…¥åº“æˆåŠŸ! å…±å¯¼å…¥ {stats['success']} ç¯‡è®ºæ–‡çš„åˆ†ææ•°æ®åˆ°çŸ¥è¯†å›¾è°±")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
    finally:
        if ingester:
            ingester.close()


if __name__ == "__main__":
    main()
